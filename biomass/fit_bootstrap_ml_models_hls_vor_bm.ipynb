{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77777f95-196e-4e66-b119-12481dbb82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, LeaveOneGroupOut, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a67c4c-c2ae-456e-af0b-9ceb0137dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask cluster location\n",
    "cluster_loc = 'local'\n",
    "prefix = 'plot'\n",
    "tuneby = 'year'\n",
    "\n",
    "retune_bootstrap = False\n",
    "drop_complex = True\n",
    "\n",
    "inDIR = '../data/training/'\n",
    "inFILE = 'vor_2013_2022_cln_2023_08_29_' + prefix + '_hls_idxs.csv'\n",
    "\n",
    "with open('results/ml_train_' + prefix + '_cv_year_tuneby_' + tuneby + '_results.pk', 'rb') as f:\n",
    "    mod_dict = pickle.load(f)\n",
    "\n",
    "inPATH = os.path.join(inDIR, inFILE)\n",
    "\n",
    "lr_mod = pickle.load(open(\"../models/biomass/CPER_HLS_to_VOR_biomass_model_lr_simp.pk\", 'rb'))\n",
    "\n",
    "outDIR = './results/'\n",
    "\n",
    "var_names = ['dfi', 'ndvi', 'ndti', 'satvi', 'ndii7', \n",
    "             'savi', 'rdvi', 'mtvi1', 'nci', 'ndci', 'psri', 'ndwi', 'evi', 'tcbi', 'tcgi', 'tcwi',\n",
    "             'blue', 'green', 'red', 'nir', 'swir1', 'swir2',\n",
    "             'bai_126', 'bai_136', 'bai_146', 'bai_236', 'bai_246', 'bai_346']\n",
    "\n",
    "var_dict = {\n",
    "    'NDVI': 'ndvi',\n",
    "    'DFI': 'dfi',\n",
    "    'NDTI': 'ndti',\n",
    "    'SATVI': 'satvi',\n",
    "    'NDII7': 'ndii7',\n",
    "    'SAVI': 'savi',\n",
    "    'RDVI': 'rdvi',\n",
    "    'MTVI1': 'mtvi1', \n",
    "    'NCI': 'nci', \n",
    "    'NDCI': 'ndci',\n",
    "    'PSRI': 'psri',\n",
    "    'NDWI': 'ndwi',\n",
    "    'EVI': 'evi',\n",
    "    'TCBI': 'tcbi',\n",
    "    'TCGI': 'tcgi',\n",
    "    'TCWI': 'tcwi',\n",
    "    'BAI_126': 'bai_126',\n",
    "    'BAI_136': 'bai_136',\n",
    "    'BAI_146': 'bai_146',\n",
    "    'BAI_236': 'bai_236',\n",
    "    'BAI_246': 'bai_246',\n",
    "    'BAI_346': 'bai_346',\n",
    "    'BLUE': 'blue',\n",
    "    'GREEN': 'green',\n",
    "    'RED': 'red',\n",
    "    'NIR1': 'nir',\n",
    "    'SWIR1': 'swir1',\n",
    "    'SWIR2': 'swir2'\n",
    "}\n",
    "\n",
    "rand_st = 2313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3313d512-b5e9-4a2e-9ee4-d817c8a04e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_corrcoef(y_obs, y_pred):\n",
    "    try:\n",
    "        corr_matrix = np.corrcoef(y_obs, y_pred)\n",
    "        corr = corr_matrix[0,1]\n",
    "        R_sq = corr**2\n",
    "    except RuntimeError:\n",
    "        R_sq = 'Error'\n",
    "    return R_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488cfa96-5911-4a96-9045-160ce81511de",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = 'threading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327a8d21-a43f-4612-b71b-f470c6f64c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vor = pd.read_csv(inPATH, parse_dates=[2, 3])\n",
    "df_vor = df_vor.rename(columns=var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30aa044-6a12-4296-a65d-d842adb701a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vor = df_vor[df_vor['Season'].isin(['June', 'October'])].copy()\n",
    "#df_vor = df_vor[df_vor['Year'] >= 2017].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3e207e-27c9-424e-a527-280c7034262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_complex:\n",
    "    mod_dict.pop('SVR')\n",
    "    mod_dict.pop('RF')\n",
    "    mod_dict.pop('GBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065d6950-8065-4393-8f78-60ebe8505024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any missing data\n",
    "len(df_vor[df_vor[var_names].isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b975bdff-3bd0-4f19-8d1c-8deec88a8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing data\n",
    "df_vor = df_vor[~df_vor[var_names].isnull().any(axis=1)].copy()\n",
    "#df_vor = df_vor[~df_vor['Id'].isin(df_vor[df_vor[var_names].isnull().any(axis=1)]['Id'].unique())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00804233-6d5c-4e88-90de-778a63816a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_all = df_vor[var_names]\n",
    "Y_var_all = df_vor['Biomass_kg_ha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d00b77-5d98-436d-b578-de283b75c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af4f39a4-8ce0-400e-9ff0-7f47f95d4386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYTHONWARNINGS\"] = 'ignore'# 'ignore::sklearn.exceptions.ConvergenceWarning:sklearn.model_selection.GridSearchCV'\n",
    "import multiprocessing\n",
    "import warnings\n",
    "multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "771aeb1b-4b2f-4387-8deb-1b4d511447c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mod_logo = LeaveOneGroupOut()\n",
    "mod_groupk = GroupKFold(n_splits=10)\n",
    "scoring = {'R2': 'r2', 'MSE': 'neg_mean_squared_error', 'MAPE': 'neg_mean_absolute_percentage_error'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a609d470-848a-4e1c-a134-7baa4f4482ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import time\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f65726b-aa92-494b-b52e-76a7b5204e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in mod_dict:\n",
    "    param_best = {}\n",
    "    param_best_dict = {p: [] for p in mod_dict[k]['param_grid'].keys()}\n",
    "    for y in mod_dict[k]['tune_results'].keys():\n",
    "        param_best_dict_tmp = mod_dict[k]['tune_results'][y]['params'][mod_dict[k]['tune_results'][y]['mean_test_' + mod_dict[k]['tune_refit']].argmax()]\n",
    "        for p in param_best_dict_tmp.keys():\n",
    "            param_best_dict[p].append(param_best_dict_tmp[p])\n",
    "    #print(param_best_dict)\n",
    "    for p in param_best_dict:\n",
    "        if all([type(i) in [float, np.float64] for i in param_best_dict[p]]):\n",
    "            param_best[p] = np.mean(param_best_dict[p])\n",
    "        elif all([type(i) in [int, np.int64] for i in param_best_dict[p]]):\n",
    "            param_best[p] = int(np.mean(param_best_dict[p]))\n",
    "        elif all([i is None for i in param_best_dict[p]]):\n",
    "            param_best[p] = None\n",
    "        else:\n",
    "            print('ERROR')\n",
    "    mod_dict[k]['param_best'] = param_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf039f2c-a111-417e-93af-67396a54134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_yrs = pd.DataFrame(columns=['Model', 'numb_yrs', 'yr_train', 'yr_test',\n",
    "                                       'MAE_kg', 'MAPE', 'MAE_pct', 'R2', 'r2_coef', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7057bdbc-da73-4045-9660-3d472a3bad34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:42<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 4-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:26<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [02:46<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 6-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [03:02<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 7-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 8-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [01:03<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 9-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10-year combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "idx_ct = 0\n",
    "for yr_n in range(3, 1 + len(df_vor['Year'].unique())):\n",
    "    print('Running ' + str(yr_n) + '-year combos')\n",
    "    combos = list(itertools.combinations(df_vor['Year'].unique(), yr_n))\n",
    "    for yr_combo in tqdm(combos):\n",
    "        df_vor_sub = df_vor[df_vor['Year'].isin(yr_combo)]\n",
    "        for train_index, test_index in mod_logo.split(df_vor_sub, groups=df_vor_sub['Year']):\n",
    "            yr = df_vor_sub['Date'].dt.year.iloc[test_index].unique()[0]\n",
    "        \n",
    "            train_loc = df_vor_sub.iloc[train_index].index\n",
    "            test_loc = df_vor_sub.iloc[test_index].index\n",
    "            \n",
    "            all_y_orig = df_vor_sub['Biomass_kg_ha'].iloc[train_index]\n",
    "            all_Y_orig = df_vor_sub['Biomass_kg_ha'].iloc[test_index]\n",
    "            all_x_orig = df_vor_sub[var_names].iloc[train_index, :]\n",
    "            all_X_orig = df_vor_sub[var_names].iloc[test_index, :]\n",
    "        \n",
    "            for k in mod_dict:\n",
    "                if mod_dict[k]['fit']:\n",
    "                    t0 = time.time()\n",
    "                    if mod_dict[k]['log_y']:\n",
    "                        all_y = np.log(1 + all_y_orig)\n",
    "                        all_Y = np.log(1 + all_Y_orig)\n",
    "                    else:\n",
    "                        all_y = all_y_orig.copy()\n",
    "                        all_Y = all_Y_orig.copy()\n",
    "                    if mod_dict[k]['scale_x']:\n",
    "                        scaler.fit(all_x_orig)\n",
    "                        all_x = scaler.transform(all_x_orig)\n",
    "                        all_X = scaler.transform(all_X_orig)\n",
    "                    else:\n",
    "                        all_x = all_x_orig.copy()\n",
    "                        all_X = all_X_orig.copy()\n",
    "                \n",
    "                    if mod_dict[k]['interactions']:\n",
    "                        poly_x = PolynomialFeatures(degree=mod_dict[k]['interaction_poly'], \n",
    "                                                    interaction_only=mod_dict[k]['interaction_only'], include_bias = False)\n",
    "                        all_x = poly_x.fit_transform(all_x)\n",
    "                        poly_X = PolynomialFeatures(degree=mod_dict[k]['interaction_poly'], \n",
    "                                                    interaction_only=mod_dict[k]['interaction_only'], include_bias = False)\n",
    "                        all_X = poly_X.fit_transform(all_X)\n",
    "                        var_names_out = poly_x.get_feature_names_out(var_names)\n",
    "                    else:\n",
    "                        var_names_out = var_names\n",
    "        \n",
    "                    # create a base model\n",
    "                    mod_base = mod_dict[k]['base_mod']\n",
    "                    # set parameters\n",
    "                    if retune_bootstrap:\n",
    "                        if tuneby == 'year':\n",
    "                            cv_splitter = mod_logo.split(all_x, groups=df_vor['Date'].dt.year.iloc[train_index])\n",
    "                        elif tuneby == 'pasture':\n",
    "                            cv_splitter = mod_groupk.split(all_x, groups=df_vor['Pasture'].iloc[train_index])\n",
    "                        grid_search = GridSearchCV(estimator=mod_base,\n",
    "                                                           param_grid=mod_dict[k]['param_grid'],\n",
    "                                                           scoring=scoring, \n",
    "                                                           refit=mod_dict[k]['tune_refit'], \n",
    "                                                           return_train_score=True,\n",
    "                                                           cv=cv_splitter, \n",
    "                                                           n_jobs=-1, \n",
    "                                                           verbose=0)\n",
    "                        with parallel_backend(backend):\n",
    "                            with warnings.catch_warnings():\n",
    "                                warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "                                grid_search.fit(all_x, all_y)\n",
    "                        mod_fnl = mod_base.set_params(**grid_search.best_params_)\n",
    "                        mod_fnl.fit(all_x, all_y)\n",
    "                    else:\n",
    "                        if mod_dict[k]['tune']:\n",
    "                            mod_fnl = mod_base.set_params(**mod_dict[k]['param_best'])\n",
    "                        else:\n",
    "                            mod_fnl = mod_base\n",
    "\n",
    "                    # fit model\n",
    "                    mod_fnl.fit(all_x, all_y)\n",
    "                \n",
    "                    if mod_dict[k]['log_y']:\n",
    "                        preds = np.exp(mod_fnl.predict(all_X).squeeze()) + 1\n",
    "                    else:\n",
    "                        preds = mod_fnl.predict(all_X).squeeze()\n",
    "                \n",
    "                    mae_kg_tmp = np.nanmean(np.abs(preds - all_Y_orig))\n",
    "                    mape_tmp = np.nanmean(np.abs(preds - all_Y_orig) / all_Y_orig)\n",
    "                    mae_pct_tmp = mae_kg_tmp / np.nanmean(all_Y_orig)\n",
    "                    r2_tmp = r2_score(all_Y_orig, preds).round(3)\n",
    "                    r2_corr_tmp = r2_corrcoef(all_Y_orig, preds).round(3)\n",
    "                    df_results_yrs = pd.concat([df_results_yrs,\n",
    "                                                pd.DataFrame({'Model': k,\n",
    "                                                              'numb_yrs': [yr_n - 1],\n",
    "                                                              'yr_train': [df_vor_sub['Date'].dt.year.iloc[train_index].unique()],\n",
    "                                                              'yr_test': yr,\n",
    "                                                              'MAE_kg': mae_kg_tmp,\n",
    "                                                              'MAPE': mape_tmp,\n",
    "                                                              'MAE_pct': mae_pct_tmp,\n",
    "                                                              'R2': r2_tmp,\n",
    "                                                              'r2_coef': r2_corr_tmp},\n",
    "                                                             index=[idx_ct])])\n",
    "                    idx_ct += 1\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f38fb0-cd0a-4014-8a36-7618768ded77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_yrs.to_csv('results/bootstrap_ml_pred_' + prefix + '_cv_year_tuneby_' + tuneby + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5a18b-099a-4ae8-a9ab-becb59674055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_nrt_env",
   "language": "python",
   "name": "hls_nrt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
